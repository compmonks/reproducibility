{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO\n",
    "- experiment: p300 speller\n",
    "- stimulation: rsvp 100ms(stim) / 75ms(blank) / 2500ms break between char / 15 flashes per char / random words of 10 chars\n",
    "- users tested: 1\n",
    "- devices tested : \n",
    "    - muse 2: freq 256Hz / channels TP9,AF7,AF8,TP10\n",
    "    - muse 2+: freq 256Hz / channels TP9,AF7,AF8,TP10,POz\n",
    "    - OpenBCI: freq 125Hz / channels FC3,FCz,FC4,T7,C3,Cz,C4,T8,P7,P3,Pz,P4,P8,O1,O2,Oz\n",
    "- metric used : Area Under the Curve (AUC)\n",
    "\n",
    "\n",
    "This code demonstrates how to deduce a best fit classifier and word length for a given user and related dataset, for a given rsvp oddball p300 speller, by progressively decreasing the length of the dataset and comparing classifier with AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILT-IN\n",
    "import os,sys\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "# DATAFRAMES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# MNE\n",
    "from mne import Epochs, find_events\n",
    "from mne.channels import read_montage\n",
    "from mne import create_info, concatenate_raws\n",
    "from mne.io import RawArray\n",
    "from mne.decoding import Vectorizer\n",
    "# SCIKIT-LEARN\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "# PYRIEMANN\n",
    "from pyriemann.estimation import ERPCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.spatialfilters import Xdawn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_folder_path = \"../data/p300_speller\" # relative datasets path\n",
    "the_user = \"compmonks\" # check available users in data folder or add new ones\n",
    "the_device = \"muse2+\" # \"muse2+\" # \"muse2\" # \"openbci_v207\" # available devices\n",
    "the_freq = 256 # 256 (Muse) # 125 (OpenBCI) # Sampling Frequency in Hertz\n",
    "break_epoch = 2500 # ms epoch used to break between character sessions\n",
    "the_montage = \"standard_1005\" # \"standard_1005\" (Muse) # \"standard_1020\" (OpenBCI) # channels montage\n",
    "the_units = \"uVolts\" # \"uVolts\" # \"Volts\" # unit of received data from device\n",
    "the_markers = {'Non-Target': 2, 'Target': 1} # markers from stim data\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "diverging_color_palette = \"coolwarm\"\n",
    "categorical_color_palette = \"Paired\"\n",
    "char_num = 10 # number of total characters in the word\n",
    "# if adding more users and data, please the data structure is adequate\n",
    "\n",
    "# list of best known discriminators for P300\n",
    "clfs = OrderedDict()\n",
    "clfs['Vect + LR'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "clfs['Vect + RegLDA'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['Xdawn + RegLDA'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['ERPCov + TS'] = make_pipeline(ERPCovariances(), TangentSpace(), LogisticRegression())\n",
    "clfs['ERPCov + MDM'] = make_pipeline(ERPCovariances(), MDM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotConditions(the_epochs, unit, channels_num, diff_waveform ,conditions = OrderedDict()):\n",
    "    \"\"\" Plot ERP conditions. \"\"\"\n",
    "\n",
    "    confidence_interval = 97.5 # confidence interval in range [0, 100]\n",
    "    bootstrap_samples = 1000 # number of bootstrap samples\n",
    "    title = 'ERP Conditions'\n",
    "    ylim = (-6,6) #(tuple): (ymin, ymax)\n",
    "    if isinstance(conditions, dict):\n",
    "        conditions = OrderedDict(conditions)\n",
    "        palette = sns.color_palette(diverging_color_palette, len(conditions))\n",
    "        if unit == \"Volts\":\n",
    "            X = the_epochs.get_data() \n",
    "        elif unit == \"uVolts\":\n",
    "            X = the_epochs.get_data() * 1e6 # convert from Volts to uVolts\n",
    "        else:\n",
    "            raise\n",
    "        times = the_epochs.times\n",
    "        y = pd.Series(the_epochs.events[:, -1])\n",
    "        # get the amount of rows and columns and set axes\n",
    "        fig_cell = math.ceil(math.sqrt(channels_num))\n",
    "        fig, axes = plt.subplots(fig_cell, fig_cell, figsize=[12, 6],sharex=True, sharey=True)\n",
    "        axes = [axes[row, col] for row in range(fig_cell) for col in range(fig_cell)]\n",
    "        # make the subplots\n",
    "        for ch in range(channels_num):\n",
    "            for cond, color in zip(conditions.values(), palette):\n",
    "                sns.tsplot(X[y.isin(cond), ch],\n",
    "                           time = times, \n",
    "                           color = color,\n",
    "                           n_boot = bootstrap_samples, \n",
    "                           ci = confidence_interval, \n",
    "                           ax = axes[ch])\n",
    "            if diff_waveform:\n",
    "                diff = (np.nanmean(X[y == diff_waveform[1], ch], axis=0) - np.nanmean(X[y == diff_waveform[0], ch], axis=0))\n",
    "                axes[ch].plot(times, diff, color='darkblue', lw=0.25)\n",
    "            axes[ch].set_title(the_epochs.ch_names[ch])\n",
    "            axes[ch].set_ylim(ylim)\n",
    "            axes[ch].axvline(x=0, ymin=ylim[0], ymax=ylim[1], color='grey',lw=0.5, label='_nolegend_')\n",
    "        axes[0].set_ylabel('Amplitude (uV)')\n",
    "        axes[-1].set_xlabel('Time (s)')\n",
    "        if diff_waveform:\n",
    "            legend = (['{} - {}'.format(diff_waveform[1], diff_waveform[0])] + list(conditions.keys()))\n",
    "        else:\n",
    "            legend = conditions.keys()\n",
    "        axes[-1].legend(legend,loc='best',fontsize='xx-small')\n",
    "        sns.despine()\n",
    "        #plt.tight_layout()\n",
    "        if title:\n",
    "            fig.suptitle(title, fontsize=20)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"bad conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentByLengthSeq(the_list,the_val,the_min_len):\n",
    "    \"\"\"Return an index of beginning of specific consecutive value sequences.\"\"\"\n",
    "    \n",
    "    out = []\n",
    "    i=0\n",
    "    ind_=0\n",
    "    for key,group in itertools.groupby(the_list):\n",
    "        ind_ = len(list(group))\n",
    "        if key == the_val and ind_>=the_min_len:\n",
    "            out.append((key,ind_,i))\n",
    "        i+=ind_\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSTPROCESS & EVALUATE DISCRIMINATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 10 characters\n",
      "device:muse2+ user:compmonks dir:session_000\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=96833\n",
      "    Range : 0 ... 96832 =      0.000 ...   378.250 secs\n",
      "Ready.\n",
      "device:muse2+ user:compmonks dir:session_001\n"
     ]
    }
   ],
   "source": [
    "all_results =  pd.DataFrame({'Temp' : []})\n",
    "the_data_path = os.path.join(the_folder_path,the_device,the_user,\"training\")\n",
    "\n",
    "# evaluate while decreasing the number of characters used\n",
    "for wl in range(char_num,0,-1):\n",
    "    raw = []\n",
    "    print(\"training with {} characters\".format(wl))\n",
    "    # loop through subdirs and concatenate data\n",
    "    for root, subdirs, files in os.walk(the_data_path):\n",
    "        for dirs in subdirs:\n",
    "            print(\"device:{} user:{} dir:{}\".format(the_device,the_user,dirs))\n",
    "            file_list = os.listdir(os.path.join(root,dirs))\n",
    "            if len(file_list) == 2:\n",
    "                if \"FEEDBACK\" in file_list[0]:\n",
    "                    dataF = pd.read_hdf(os.path.join(root,dirs,file_list[0]),'data')\n",
    "                    dataA = pd.read_hdf(os.path.join(root,dirs,file_list[1]),'EEG')\n",
    "                else:\n",
    "                    dataF = pd.read_hdf(os.path.join(root,dirs,file_list[1]),'data')\n",
    "                    dataA = pd.read_hdf(os.path.join(root,dirs,file_list[0]),'EEG')\n",
    "                #\n",
    "                if wl != char_num:\n",
    "                    #get a chunk of the data depending of the number of charactersw to use\n",
    "                    chunks_indexes = segmentByLengthSeq(dataF['index'],12.0,(the_freq * (break_epoch/1000))*0.9)\n",
    "                    #trim signal accordingly\n",
    "                    stop_F = dataF.index[chunks_indexes[wl][2]]\n",
    "                    stop_A = dataA.index.get_loc(stop_F,method='ffill')\n",
    "                    dataA = dataA.iloc[:stop_A]\n",
    "                    #\n",
    "                dataA['Stim'] = np.nan\n",
    "                prev_marker = 0\n",
    "                for index,row in dataA.iterrows():\n",
    "                    new_marker = dataF.iloc[dataF.index.get_loc(pd.to_datetime(index),method='nearest')]['marker'].astype(int)\n",
    "                    if prev_marker == 0:\n",
    "                        if new_marker != 0:\n",
    "                            row['Stim'] = new_marker\n",
    "                            prev_marker = new_marker\n",
    "                        else:\n",
    "                            row['Stim'] = 0\n",
    "                    else:\n",
    "                        row['Stim'] = 0\n",
    "                        prev_marker = new_marker\n",
    "                channel_names = list(dataA.keys())\n",
    "                channel_types = ['eeg'] * (len(list(dataA.keys()))-1) + ['stim']\n",
    "                the_data = dataA.values[:].T\n",
    "                the_data[:-1] *= 1e-6 if the_units == \"uVolts\" else 1\n",
    "                info = create_info(ch_names = channel_names, \n",
    "                                   ch_types = channel_types,\n",
    "                                   sfreq = the_freq, \n",
    "                                   montage = the_montage)\n",
    "                raw.append(RawArray(data = the_data, info = info))\n",
    "                #\n",
    "    the_raw = concatenate_raws(raw)\n",
    "    # plot PSD on raw data\n",
    "    the_raw.plot_psd()\n",
    "    # bandpass filter\n",
    "    the_raw=the_raw.filter(.1,20, method='iir',iir_params = dict(order=8,ftype='butter',output='sos'))\n",
    "    # plot PSD on filtered data\n",
    "    the_raw.plot_psd(fmin=.1, fmax=20)\n",
    "    # events with artifact rejection\n",
    "    the_events = find_events(the_raw)\n",
    "    the_epochs = Epochs(the_raw,\n",
    "                        events = the_events, \n",
    "                        event_id = the_markers,\n",
    "                        tmin = -0.1, tmax = 0.7, # tmin and tmax are in s. ie. 100ms before and 800ms after\n",
    "                        baseline = None, # no baseline correction necessary after bandpass\n",
    "                        reject = {'eeg': 75e-6}, # remove amplitudes above 75uV ie. eye blinks\n",
    "                        preload = True,\n",
    "                        verbose = False,\n",
    "                        picks = list(range(len(channel_names)-1))\n",
    "                       )\n",
    "    the_sample_drop = (1 - len(the_epochs.events)/len(the_events)) * 100\n",
    "    print(\"sample drop: {} %\".format(the_sample_drop))\n",
    "    # plot events conditions\n",
    "    conditions = OrderedDict()\n",
    "    conditions['2: Non-target'] = [the_markers[\"Non-Target\"]]\n",
    "    conditions['1: Target'] = [the_markers[\"Target\"]]\n",
    "    if len(the_epochs.events)>0:\n",
    "        plotConditions(the_epochs, the_units, len(channel_names)-1, diff_waveform = (2,1) ,conditions = conditions)\n",
    "        plt.savefig(os.path.join(the_data_path,\"CONDITIONS.png\"),dpi = 800,format = \"png\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(\"---> dataset too noisy to find any events\")\n",
    "    # prepare data for training\n",
    "    the_epochs.pick_types(eeg=True)\n",
    "    X = the_epochs.get_data() * 1e6\n",
    "    times = the_epochs.times\n",
    "    y = the_epochs.events[:, -1]\n",
    "    cv = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=42)\n",
    "    auc = []\n",
    "    methods = []\n",
    "    # cross validation AUC score by classifier\n",
    "    for m in clfs:\n",
    "        res = cross_val_score(clfs[m], X, y==2, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "        auc.extend(res)\n",
    "        methods.extend([m]*len(res))\n",
    "    # plot\n",
    "    results = pd.DataFrame(data=auc, columns=['AUC'])\n",
    "    results['Method'] = methods\n",
    "    results['char_num'] = wl\n",
    "    palette = sns.color_palette(categorical_color_palette, len(results))\n",
    "    plt.figure(figsize=[10,4])\n",
    "    plt.title('Predictor Evaluation for AUC')\n",
    "    sns.barplot(data=results, x='AUC', y='Method', palette=palette)\n",
    "    plt.xlim(0.2, 0.99)\n",
    "    sns.despine()\n",
    "    #\n",
    "    the_best_score = max(auc)\n",
    "    the_best_method = methods[auc.index(the_best_score)]\n",
    "    print(\"The best predictor is: {} score:{}\".format(the_best_method, the_best_score))\n",
    "    # concatenating all results\n",
    "    if all_results.empty:\n",
    "        all_results = results.copy()\n",
    "    else:\n",
    "        all_results = pd.concat([all_results, results], ignore_index=True, sort=False)\n",
    "# plotting the accuracy comparison over characters amount\n",
    "sns.catplot(x='char_num', y='AUC', hue='Method', data=all_results, kind='bar',palette=palette,aspect=4)\n",
    "#plt.savefig(os.path.join(the_data_path,\"AUC.png\"),dpi = 800,format = \"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
