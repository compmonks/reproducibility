{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO\n",
    "- experiment: p300 speller\n",
    "\n",
    "This code demonstrates how to predict  a spelled word from a trained discriminator and unlabelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILT-IN\n",
    "import os,sys\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "# DATAFRAMES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# MNE\n",
    "from mne import Epochs, find_events\n",
    "from mne.channels import read_montage\n",
    "from mne import create_info, concatenate_raws\n",
    "from mne.io import RawArray\n",
    "from mne.decoding import Vectorizer\n",
    "# SCIKIT-LEARN\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import Imputer\n",
    "# PYRIEMANN\n",
    "from pyriemann.estimation import ERPCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.spatialfilters import Xdawn\n",
    "\n",
    "\n",
    "\n",
    "from itertools import starmap, product\n",
    "from operator import and_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_folder_path = \"../data/p300_speller\" # relative datasets path\n",
    "the_user = \"compmonks\" # \"all\" # check available users in data folder or add new ones\n",
    "the_device = \"muse2+\" # \"muse2+\" # \"muse2\" # \"openbci_v207\" # available devices\n",
    "the_freq = 256 # 256 (Muse) # 125 (OpenBCI) # Sampling Frequency in Hertz\n",
    "the_montage = \"standard_1005\" # \"standard_1005\" (Muse) # \"standard_1020\" (OpenBCI) # channels montage\n",
    "the_units = \"uVolts\" # \"uVolts\" # \"Volts\" # unit of received data from device\n",
    "the_markers = {'Non-Target': 2, 'Target': 1} # markers from stim data\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "diverging_color_palette = \"coolwarm\"\n",
    "categorical_color_palette = \"Paired\"\n",
    "char_num = 10 # number of total characters in the word\n",
    "# if adding more users and data, please the data structure is adequate\n",
    "break_epoch = 2500\n",
    "row_col = ['AGMSY5', #0\n",
    "           'BHNTZ6',\n",
    "           'CIOU17',\n",
    "           'DJPV28',\n",
    "           'EKQW39',\n",
    "           'FLRX40',\n",
    "           'ABCDEF',\n",
    "           'GHIJKL',\n",
    "           'MNOPQR',\n",
    "           'STUVWX',\n",
    "           'YZ1234',\n",
    "           '567890'] #11\n",
    "            # 12 is blank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentByLengthSeq(the_list,the_val,the_min_len):\n",
    "    \"\"\"Return an index of beginning of specific consecutive value sequences.\"\"\"\n",
    "    \n",
    "    out = []\n",
    "    i=0\n",
    "    ind_=0\n",
    "    for key,group in itertools.groupby(the_list):\n",
    "        ind_ = len(list(group))\n",
    "        if key == the_val and ind_>=the_min_len:\n",
    "            out.append((key,ind_,i))\n",
    "        i+=ind_\n",
    "        \n",
    "    return out\n",
    "\n",
    "def all_intersections(sets):\n",
    "    # Convert to set of frozensets for uniquification/type correctness\n",
    "    last = new = sets = set(map(frozenset, sets))\n",
    "    # Keep going until further intersections add nothing to results\n",
    "    while new:\n",
    "        # Compute intersection of old values with newly found values\n",
    "        new = set(starmap(and_, product(last, new)))\n",
    "        last = sets.copy()  # Save off prior state\n",
    "        new -= last         # Determine truly newly added values\n",
    "        sets |= new         # Accumulate newly added values in complete set\n",
    "    # No more intersections being generated, convert results to canonical\n",
    "    # form, list of lists, where each sublist is displayed in order, and\n",
    "    # the top level list is ordered first by size of sublist, then by contents\n",
    "    return sorted(map(sorted, sets), key=lambda x: (len(x), x))\n",
    "    \n",
    "def checkSpelledAnswer(answers,flashed,tokens):\n",
    "    \"\"\"...\"\"\"\n",
    "    \n",
    "    intersections = dict()\n",
    "    selected_tokens = []\n",
    "    # FILTER PREDICTED ROWS AND COLS\n",
    "    for i,val in enumerate(answers):\n",
    "        mask = val==1\n",
    "        filtered = flashed[i][mask]\n",
    "        filtered = np.unique(filtered)\n",
    "        if filtered.size > 0:\n",
    "            selected_tokens.append(set(tokens[filtered[0]]))\n",
    "    for rc in  selected_tokens:\n",
    "        for _char in rc:\n",
    "            intersections[_char]=intersections.setdefault(_char,0)+1\n",
    "            \n",
    "    #print(\"selected_tokens:{}\".format(selected_tokens))\n",
    "    #the_char = set.intersection(*selected_tokens)\n",
    "    #print(\"intersection:{}\".format(set.intersection(*selected_tokens)))\n",
    "    _sorted = sorted(intersections.items(), key=lambda kv: kv[1])\n",
    "    #print(\"intersection:{}\".format(_sorted[-1]))\n",
    "    print(\"FOUND CHARACTER: {}\".format(_sorted[-1][0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:muse2+ user:compmonks dir:debug\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8359\n",
      "    Range : 0 ... 8358 =      0.000 ...    32.648 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 2 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "178 events found\n",
      "Event IDs: [1 2]\n",
      "178 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 178 events and 206 original time points ...\n",
      "4 bad epochs dropped\n",
      "FOUND CHARACTER: O\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8405\n",
      "    Range : 0 ... 8404 =      0.000 ...    32.828 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 2 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Event IDs: [1 2]\n",
      "179 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 179 events and 206 original time points ...\n",
      "4 bad epochs dropped\n",
      "FOUND CHARACTER: U\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8406\n",
      "    Range : 0 ... 8405 =      0.000 ...    32.832 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 1 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Event IDs: [1 2]\n",
      "179 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 179 events and 206 original time points ...\n",
      "4 bad epochs dropped\n",
      "FOUND CHARACTER: D\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8406\n",
      "    Range : 0 ... 8405 =      0.000 ...    32.832 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 2 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Event IDs: [1 2]\n",
      "179 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 179 events and 206 original time points ...\n",
      "4 bad epochs dropped\n",
      "FOUND CHARACTER: Q\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8407\n",
      "    Range : 0 ... 8406 =      0.000 ...    32.836 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 2 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Event IDs: [1 2]\n",
      "179 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 179 events and 206 original time points ...\n",
      "4 bad epochs dropped\n",
      "FOUND CHARACTER: S\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8406\n",
      "    Range : 0 ... 8405 =      0.000 ...    32.832 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 2 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Event IDs: [1 2]\n",
      "179 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 179 events and 206 original time points ...\n",
      "4 bad epochs dropped\n",
      "FOUND CHARACTER: I\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8406\n",
      "    Range : 0 ... 8405 =      0.000 ...    32.832 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 2 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Event IDs: [1 2]\n",
      "179 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 179 events and 206 original time points ...\n",
      "4 bad epochs dropped\n",
      "FOUND CHARACTER: D\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8462\n",
      "    Range : 0 ... 8461 =      0.000 ...    33.051 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 1 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Event IDs: [1 2]\n",
      "179 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 179 events and 206 original time points ...\n",
      "4 bad epochs dropped\n",
      "FOUND CHARACTER: J\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8406\n",
      "    Range : 0 ... 8405 =      0.000 ...    32.832 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 1 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "179 events found\n",
      "Event IDs: [1 2]\n",
      "179 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 179 events and 206 original time points ...\n",
      "4 bad epochs dropped\n",
      "FOUND CHARACTER: N\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=8396\n",
      "    Range : 0 ... 8395 =      0.000 ...    32.793 secs\n",
      "Ready.\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "Trigger channel has a non-zero initial value of 2 (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "178 events found\n",
      "Event IDs: [1 2]\n",
      "178 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 178 events and 206 original time points ...\n",
      "3 bad epochs dropped\n",
      "FOUND CHARACTER: G\n"
     ]
    }
   ],
   "source": [
    "the_data_path = os.path.join(the_folder_path,the_device,the_user) if the_user != \"all\" else os.path.join(the_folder_path,the_device)\n",
    "\n",
    "the_predictor = joblib.load(os.path.join(the_data_path,\"compmonks_T2_erp-cov-ts.pkl\"),\"r\")\n",
    "# loop through subdirs and concatenate data\n",
    "for root, subdirs, files in os.walk(the_data_path):\n",
    "    for dirs in subdirs:\n",
    "        print(\"device:{} user:{} dir:{}\".format(the_device,the_user,dirs))\n",
    "        file_list = os.listdir(os.path.join(root,dirs))\n",
    "        for _f in file_list:\n",
    "            if len(_f.split('.')[0]) <= 1 or \".pkl\" in _f:\n",
    "                file_list.remove(_f)\n",
    "        #print(\"files:{}\".format(file_list))\n",
    "        if len(file_list) == 2:\n",
    "            if \"FEEDBACK\" in file_list[0]:\n",
    "                dataF = pd.read_hdf(os.path.join(root,dirs,file_list[0]),'data')\n",
    "                dataA = pd.read_hdf(os.path.join(root,dirs,file_list[1]),'EEG')\n",
    "            else:\n",
    "                dataF = pd.read_hdf(os.path.join(root,dirs,file_list[1]),'data')\n",
    "                dataA = pd.read_hdf(os.path.join(root,dirs,file_list[0]),'EEG')\n",
    "        # BREAK DATA BY CHARACTER _____________________________________________________\n",
    "        # the break length between char epochs\n",
    "        the_char_epoch = (the_freq * (break_epoch/1000))*0.9\n",
    "        chunks_indexes = segmentByLengthSeq(dataF['index'],12.0,the_char_epoch)\n",
    "        #print(\"chunks_indexes:{}\".format(chunks_indexes))\n",
    "        for i,_char in enumerate(chunks_indexes):\n",
    "            raw = []\n",
    "            #\n",
    "            start_F = dataF.index[_char[2]+_char[1]]\n",
    "            stop_F =  dataF.index[chunks_indexes[i+1][2] if i+1 < len(chunks_indexes) else -1]\n",
    "            start_A = dataA.index.get_loc(start_F,method='ffill')\n",
    "            stop_A = dataA.index.get_loc(stop_F,method='ffill')\n",
    "            #\n",
    "            #print(\"start:{}\".format(dataA.index[start_A]))\n",
    "            #print(\"stop:{}\".format(dataA.index[stop_A]))\n",
    "            #\n",
    "            chunk_dataF = dataF.iloc[dataF.index.get_loc(start_F,method='ffill'):dataF.index.get_loc(stop_F,method='ffill')][:]\n",
    "            chunk_dataA = dataA.iloc[start_A:stop_A][:]\n",
    "            chunk_dataA['Stim'] = np.nan\n",
    "            prev_marker = 0\n",
    "            prev_flash = 12\n",
    "            index_list = []\n",
    "            # TO CHECK ALSO IN TRAINING: BETTER MARKING FOR EVENTS AND CHAR INDEXES\n",
    "            for index,row in chunk_dataA.iterrows():\n",
    "                new_marker = chunk_dataF.iloc[chunk_dataF.index.get_loc(pd.to_datetime(index),method='nearest')]['marker'].astype(int)\n",
    "                if prev_marker == 0:\n",
    "                    if new_marker != 0:\n",
    "                        row['Stim'] = new_marker\n",
    "                        prev_marker = new_marker\n",
    "                        new_flash = chunk_dataF['index'][chunk_dataF.index.get_loc(pd.to_datetime(index),method='nearest')].astype(int)\n",
    "                        index_list.append(new_flash)\n",
    "                    else:\n",
    "                        row['Stim'] = 0\n",
    "                else:\n",
    "                    row['Stim'] = 0\n",
    "                    prev_marker = new_marker\n",
    "                \n",
    "            channel_names = list(chunk_dataA.keys())\n",
    "            channel_types = ['eeg'] * (len(list(chunk_dataA.keys()))-1) + ['stim']\n",
    "            \n",
    "            the_data = chunk_dataA.values[:].T\n",
    "            the_data[:-1] *= 1e-6 if the_units == \"uVolts\" else 1\n",
    "\n",
    "            info = create_info(ch_names = channel_names, \n",
    "                               ch_types = channel_types,\n",
    "                               sfreq = the_freq, \n",
    "                               montage = the_montage)\n",
    "            raw.append(RawArray(data = the_data, info = info))\n",
    "            the_raw = concatenate_raws(raw)\n",
    "\n",
    "            the_raw=the_raw.filter(.1,20, method='iir',iir_params = dict(order=8,ftype='butter',output='sos'))\n",
    "            \n",
    "            # events with artifact rejection\n",
    "            the_events = find_events(the_raw)\n",
    "            # remove first index if orphaned data found first during find_events\n",
    "            if the_events[0][0] != 0:\n",
    "                index_list = index_list[1:]\n",
    "            \n",
    "            #print(\"unique event codes:{}\".format(np.unique(the_events[:,2])))\n",
    "            the_epochs = Epochs(the_raw,\n",
    "                                events = the_events, \n",
    "            #                    event_id = None,\n",
    "                                tmin = -0.1, tmax = 0.7, # tmin and tmax are in s. ie. 100ms before and 800ms after\n",
    "                                baseline = None, # no baseline correction necessary after bandpass\n",
    "            #                    reject = {'eeg': 75e-6}, # remove amplitudes above 75uV ie. eye blinks\n",
    "                                preload = True,\n",
    "            #                    verbose = False,\n",
    "                                picks = list(range(len(channel_names)-1))\n",
    "                               )\n",
    "            # pick indexes according to selected epochs\n",
    "            index_list = [index_list[i] for i in the_epochs.selection]\n",
    "            #print(\"Flash length:{}\".format(len(index_list)))\n",
    "            #print(\"Flash indexes:{}\".format(index_list))\n",
    "            \n",
    "            #the_epochs = the_epochs.pick_types(eeg=True)\n",
    "            X = the_epochs.get_data()# * 1e6\n",
    "            preds = the_predictor.predict(X)\n",
    "            #print(\"preds:{}\".format(preds))\n",
    "            #print(\"preds shape:{}\".format(preds.shape))\n",
    "            \n",
    "            checkSpelledAnswer(preds,index_list,row_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
