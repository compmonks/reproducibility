{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO\n",
    "- experiment: p300 speller\n",
    "- data preprocessing is similar to p300_speller_training\n",
    "\n",
    "This code demonstrates how to predict  a spelled word from a trained discriminator and unlabelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILT-IN\n",
    "import os\n",
    "from itertools import starmap, product, groupby\n",
    "from operator import and_\n",
    "import math\n",
    "\n",
    "# DATAFRAMES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# MNE\n",
    "from mne import Epochs, find_events\n",
    "from mne import create_info, concatenate_raws\n",
    "from mne.io import RawArray\n",
    "\n",
    "# SCIKIT-LEARN\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = {\"openbci_v207\":\"compmonks_T2_ERPCov-TS_OPENBCI-Cytonv207_2019-7-11_15-10-00-000000.pkl\",\n",
    "              \"muse2\":\"compmonks_T2_Vect-LR_INTERAXON-Muse2_2019-5-28_7-3-46-877810_0sPyS4R.pkl\",\n",
    "              \"muse2+\":\"compmonks_T2_Vect-LR_INTERAXON-Muse2_2019-5-27_9-46-35-174958.pkl\"\n",
    "             }\n",
    "the_folder_path = \"../data/p300_speller\" # relative datasets path\n",
    "the_user = \"compmonks\" # check available users in data folder or add new ones\n",
    "the_device = \"openbci_v207\" # \"muse2+\" # \"muse2\" # \"openbci_v207\" # available devices\n",
    "the_freq = 125 # 256 (Muse) # 125 (OpenBCI) # Sampling Frequency in Hertz\n",
    "the_units = \"uVolts\" # \"uVolts\" # \"Volts\" # unit of received data from device\n",
    "the_montage = \"standard_1020\" # \"standard_1005\" (Muse) # \"standard_1020\" (OpenBCI) # channels montage\n",
    "the_predictor_name = predictors[the_device] # trained discriminator in the data folder\n",
    "break_epoch = 2500 # time in ms of break between each character spelling sessions\n",
    "blank_index = 12 # index used to mark a non-flashing time\n",
    "#\n",
    "# list of tokens ie. flashed rows and cols\n",
    "row_col = ['AGMSY5', #0\n",
    "           'BHNTZ6',\n",
    "           'CIOU17',\n",
    "           'DJPV28',\n",
    "           'EKQW39',\n",
    "           'FLRX40',\n",
    "           'ABCDEF',\n",
    "           'GHIJKL',\n",
    "           'MNOPQR',\n",
    "           'STUVWX',\n",
    "           'YZ1234',\n",
    "           '567890'] #11\n",
    "            # 12 is blank\n",
    "word_length = 4 # unique word length used for sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentByLengthSeq(the_list,the_val,the_min_len):\n",
    "    \"\"\"Return an index of beginning of specific consecutive value sequences.\"\"\"\n",
    "    \n",
    "    out = []\n",
    "    i=0\n",
    "    ind_=0\n",
    "    for key,group in groupby(the_list):\n",
    "        ind_ = len(list(group))\n",
    "        if key == the_val and ind_>=the_min_len:\n",
    "            out.append((key,ind_,i))\n",
    "        i+=ind_\n",
    "        \n",
    "    return out\n",
    "#\n",
    "def all_intersections(sets):\n",
    "    \"\"\"Convert to set of frozensets for uniquification/type correctness.\"\"\"\n",
    "    \n",
    "    last = new = sets = set(map(frozenset, sets))\n",
    "    while new:\n",
    "        new = set(starmap(and_, product(last, new)))\n",
    "        last = sets.copy()\n",
    "        new -= last\n",
    "        sets |= new\n",
    "        \n",
    "    return sorted(map(sorted, sets), key=lambda x: (len(x), x))\n",
    "#    \n",
    "def checkSpelledAnswer(answers,flashed,tokens):\n",
    "    \"\"\"Return a token from a list with the most labelled intersections.\"\"\"\n",
    "    \n",
    "    intersections = dict()\n",
    "    selected_tokens = []\n",
    "    for i,val in enumerate(answers):\n",
    "        mask = val==1\n",
    "        filtered = flashed[i][mask]\n",
    "        filtered = np.unique(filtered)\n",
    "        if filtered.size > 0:\n",
    "            selected_tokens.append(set(tokens[filtered[0]]))\n",
    "    for rc in  selected_tokens:\n",
    "        for _char in rc:\n",
    "            intersections[_char]=intersections.setdefault(_char,0)+1\n",
    "    _sorted = sorted(intersections.items(), key=lambda kv: kv[1])\n",
    "    if len(_sorted)>0:\n",
    "        return \"{}\".format(_sorted[-1][0])\n",
    "    else:\n",
    "        return \"#\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVE WORD FROM SPELLED CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_data_path = os.path.join(the_folder_path,the_device,the_user,\"testing\")\n",
    "the_predictor = joblib.load(os.path.join(the_data_path,the_predictor_name),\"r\")\n",
    "# loop through subdirs and concatenate data\n",
    "for root, subdirs, files in os.walk(the_data_path):\n",
    "    for dirs in subdirs:\n",
    "        the_word = \"\"\n",
    "        print(\"device:{} user:{} dir:{}\".format(the_device,the_user,dirs))\n",
    "        file_list = os.listdir(os.path.join(root,dirs))\n",
    "        for _f in file_list:\n",
    "            if len(_f.split('.')[0]) <= 1 or \".pkl\" in _f:\n",
    "                file_list.remove(_f)\n",
    "        print(\"file_list:{}\".format(file_list))\n",
    "        if len(file_list) == 2:\n",
    "            if \"FEEDBACK\" in file_list[0]:\n",
    "                dataF = pd.read_hdf(os.path.join(root,dirs,file_list[0]),'data')\n",
    "                dataA = pd.read_hdf(os.path.join(root,dirs,file_list[1]),'EEG')\n",
    "            else:\n",
    "                dataF = pd.read_hdf(os.path.join(root,dirs,file_list[1]),'data')\n",
    "                dataA = pd.read_hdf(os.path.join(root,dirs,file_list[0]),'EEG')\n",
    "        # break down dataset by character session\n",
    "        the_char_epoch = math.floor((the_freq * (break_epoch/1000))*0.9)\n",
    "        dataF = dataF[(dataF['index'].astype(str) != \"undefined\") & (dataF['index'].notna())]\n",
    "        dataF['index'] = dataF['index'].apply(np.float64)\n",
    "        chunks_indexes = segmentByLengthSeq(dataF['index'],blank_index,the_char_epoch) # 12 is the index\n",
    "        #print(\"chunks_indexes:{}\".format(chunks_indexes))\n",
    "        for i,_char in enumerate(chunks_indexes):\n",
    "            if i+1 <= word_length:\n",
    "                raw = []\n",
    "                start_F = dataF.index[_char[2]+_char[1]]\n",
    "                stop_F =  dataF.index[chunks_indexes[i+1][2] if i+1 < word_length else -1]\n",
    "                start_A = dataA.index.get_loc(start_F,method='ffill')\n",
    "                stop_A = dataA.index.get_loc(stop_F,method='ffill')\n",
    "                chunk_dataF = dataF.iloc[dataF.index.get_loc(start_F,method='ffill'):dataF.index.get_loc(stop_F,method='ffill')][:]\n",
    "                chunk_dataA = dataA.iloc[start_A:stop_A][:]\n",
    "                chunk_dataA['Stim'] = np.nan\n",
    "                prev_marker = 0\n",
    "                prev_flash = blank_index\n",
    "                index_list = []\n",
    "                for index,row in chunk_dataA.iterrows():\n",
    "                    new_marker = int(chunk_dataF.iloc[chunk_dataF.index.get_loc(pd.to_datetime(index),method='nearest')]['marker'])\n",
    "                    if prev_marker == 0:\n",
    "                        if new_marker != 0:\n",
    "                            row['Stim'] = new_marker\n",
    "                            prev_marker = new_marker\n",
    "                            new_flash = chunk_dataF['index'][chunk_dataF.index.get_loc(pd.to_datetime(index),method='nearest')].astype(int)\n",
    "                            index_list.append(new_flash)\n",
    "                        else:\n",
    "                            row['Stim'] = 0\n",
    "                    else:\n",
    "                        row['Stim'] = 0\n",
    "                        prev_marker = new_marker\n",
    "                channel_names = list(chunk_dataA.keys())\n",
    "                chunk_dataA['Stim'] = chunk_dataA['Stim'].astype(int)          \n",
    "                channel_types = ['eeg'] * (len(list(chunk_dataA.keys()))-1) + ['stim']\n",
    "                the_data = chunk_dataA.values[:].T\n",
    "                the_data[:-1] *= 1e-6 if the_units == \"uVolts\" else 1\n",
    "                info = create_info(ch_names = channel_names, \n",
    "                                   ch_types = channel_types,\n",
    "                                   sfreq = the_freq, \n",
    "                                   montage = the_montage)\n",
    "                raw.append(RawArray(data = the_data, info = info))\n",
    "                the_raw = concatenate_raws(raw)\n",
    "                the_raw=the_raw.filter(.1,20, method='iir',iir_params = dict(order=20,ftype='butter',output='sos'))\n",
    "                # plot psd\n",
    "                #the_raw.plot_psd(fmin=.1, fmax=20)\n",
    "                the_events = find_events(the_raw,initial_event=True)           \n",
    "                the_epochs = Epochs(the_raw,\n",
    "                                    events = the_events, \n",
    "                                    event_id = None,\n",
    "                                    tmin = -0.1, tmax = 0.7,\n",
    "                                    baseline = None,\n",
    "                                    reject = {'eeg': 75e-6}, # remove amplitudes above 75uV ie. eye blinks\n",
    "                                    preload = True,\n",
    "                                    verbose = False,\n",
    "                                    picks = list(range(len(channel_names)-1))\n",
    "                                   )\n",
    "                # epoch averaging\n",
    "                #the_epochs.average() # inplace\n",
    "                #the_sample_drop = (1 - len(the_epochs.events)/len(the_events)) * 100\n",
    "                #print(\"sample drop: {} %\".format(the_sample_drop))\n",
    "                # pick indexes according to selected epochs\n",
    "                index_list = [index_list[i] for i in the_epochs.selection]\n",
    "                the_epochs = the_epochs.pick_types(eeg=True)\n",
    "                X = the_epochs.get_data() * 1e6\n",
    "                #print(\"X shape:{}\".format(X.shape))\n",
    "                preds = the_predictor.predict(X)\n",
    "                #print(preds)\n",
    "                # relabel epoch events with predictions\n",
    "                the_epochs.events[:,-1] = preds\n",
    "                # retrieve the spelled character\n",
    "                the_word += checkSpelledAnswer(preds,index_list,row_col)\n",
    "        #\n",
    "        print(\"--->      DID YOU SPELL: {} ?\".format(the_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
