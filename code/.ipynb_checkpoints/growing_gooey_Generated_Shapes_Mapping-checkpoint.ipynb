{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO\n",
    "- experiment: growing gooey\n",
    "- stimulation: rsvp 100ms(stim) / 75ms(blank) / 2500ms break between tokens / 15 flashes per char / 6 tokens max per shape for a max time of 18 seconds.\n",
    "\n",
    "- users tested: 1\n",
    "- devices tested : \n",
    "    - OpenBCI: freq 125Hz / channels FC3,FCz,FC4,T7,C3,Cz,C4,T8,P7,P3,Pz,P4,P8,O1,O2,Oz\n",
    "- metric used : see Settings below\n",
    "\n",
    "\n",
    "This code demonstrates How to apply dimensionality reduction to generated objects of the same category, with or without several classes, with T-SNE and UMAP for comparison. Parameters may be adjusted to show more general or more local data structures. An Engagement index, computed from data frequency bands is overlayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILT-IN\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# MNE\n",
    "from mne import create_info, concatenate_raws, read_epochs,Epochs, find_events\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "from mne.io import RawArray, read_raw_fif\n",
    "\n",
    "# SCIPY\n",
    "#from scipy.signal import welch\n",
    "from scipy.integrate import simps\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# DATAFRAMES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PLOTTING\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_folder_path = os.path.join(\"..\",\"data\",\"growing_gooey\") # relative datasets path\n",
    "the_user = \"compmonks\" # check available users in data folder or add new ones\n",
    "the_device = \"openbci_v207\" # available devices\n",
    "the_data_path = os.path.join(the_folder_path,the_device,the_user)\n",
    "the_freq = 125 # 125 (OpenBCI) # Sampling Frequency in Hertz\n",
    "the_units = \"uVolts\" # \"uVolts\" # \"Volts\" # unit of received data from device\n",
    "the_montage = \"standard_1020\" # \"standard_1020\" (OpenBCI) # channels montage\n",
    "the_markers = {'Non-Target': 2, 'Target': 1} # markers from stim data\n",
    "# EEG bands\n",
    "delta_band = [.5,4.]\n",
    "theta_band = [4.,8.]\n",
    "alpha_band = [8.,12.]\n",
    "beta_band = [12.,30.]\n",
    "gamma_band = [30.,100.]\n",
    "K_ratio = 0.55 # Beta ratio apllied after bandpass filtering\n",
    "#styling\n",
    "sns.set_context('paper')\n",
    "sns.set_style('darkgrid')\n",
    "diverging_color_palette = \"coolwarm\"\n",
    "categorical_color_palette = \"Paired\"\n",
    "index_color_palette = \"viridis\"\n",
    "# list of different metrics for clustering comparison\n",
    "all_metrics = [\n",
    "                \"canberra\",\n",
    "                \"correlation\",\n",
    "                \"euclidean\",\n",
    "                \"hamming\",\n",
    "                \"manhattan\",\n",
    "                \"minkowski\",\n",
    "                \"russellrao\",\n",
    "                \"seuclidean\",\n",
    "                \"sqeuclidean\",\n",
    "                ]\n",
    "single_metric = [\"euclidean\"]\n",
    "min_dist = 1.\n",
    "n_neighbors = [15,50]\n",
    "learning_rate = 10.\n",
    "n_epochs = 5000\n",
    "random_state=42\n",
    "spread = 20.\n",
    "n_components=2\n",
    "num_noise = 1000\n",
    "metric_type = \"single\" # \"single\": will use single_metric # \"all\": will use al listed metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFile(the_path):\n",
    "    \"\"\"Load JSON file and return its data.\"\"\"\n",
    "    \n",
    "    with open(\"{}\".format(the_path),'r') as data_file:\n",
    "        load_data = json.load(data_file)\n",
    "        the_data = np.empty([len(load_data),len(load_data[0])])\n",
    "        for i,elem in enumerate(load_data):\n",
    "            the_data[i] = np.asarray(elem)\n",
    "            \n",
    "    return the_data\n",
    "   \n",
    "#\n",
    "def applyUMAP(the_data,the_nb,the_metric):\n",
    "    \"\"\"Apply UMAP dimensionality reduction (2D) to given data.\"\"\"\n",
    "    \n",
    "    map_coords = {}\n",
    "    df = pd.DataFrame(data = the_data)\n",
    "    # remove constant columns\n",
    "    df = df.loc[:, (df != df.iloc[0]).any()]\n",
    "    for _col in df:\n",
    "        if df[_col].round(3).nunique() <= 2:\n",
    "            df = df.drop([_col], axis=1)\n",
    "    # apply UMAP\n",
    "    reducer = umap.UMAP(\n",
    "                        random_state=random_state, \n",
    "                        n_epochs = n_epochs, \n",
    "                        learning_rate = learning_rate, \n",
    "                        n_neighbors = the_nb, \n",
    "                        min_dist = min_dist,\n",
    "                        spread = spread,\n",
    "                        n_components=n_components, \n",
    "                        metric = the_metric\n",
    "                       )\n",
    "    reduced = reducer.fit_transform(df)\n",
    "    # get 2d coords\n",
    "    for i,coords in enumerate(reduced):\n",
    "        map_coords[\"{}\".format(i)] = reduced[i]\n",
    "        \n",
    "    return map_coords\n",
    "#\n",
    "def applyTSNE(the_data, the_perplexity,the_metric):\n",
    "    \"\"\"Apply T-SNE dimensionality reduction (2D) to given data.\"\"\"\n",
    "    \n",
    "    map_coords = {}\n",
    "    df = pd.DataFrame(data = the_data)\n",
    "    # remove constant columns\n",
    "    df = df.loc[:, (df != df.iloc[0]).any()]\n",
    "    for _col in df:\n",
    "        if df[_col].round(3).nunique() <= 2:\n",
    "            df = df.drop([_col], axis=1)\n",
    "    # apply TSNE\n",
    "    reducer = TSNE(n_components=n_components,\n",
    "                   early_exaggeration=min_dist,\n",
    "                   perplexity=the_perplexity,\n",
    "                   learning_rate=learning_rate,\n",
    "                   n_iter=n_epochs,\n",
    "                   metric=the_metric,\n",
    "                   random_state=random_state)\n",
    "    reduced = reducer.fit_transform(df)\n",
    "    # get 2d coords\n",
    "    for i,coords in enumerate(reduced):\n",
    "        map_coords[\"{}\".format(i)] = reduced[i]\n",
    "        \n",
    "    return map_coords\n",
    "#\n",
    "def plotMap(the_type,the_coords, prop_index,the_properties,data_length,param=None, save_plot=False):\n",
    "    \"\"\"Scatterplot of clustered data points with proximity gradients.\"\"\"\n",
    "        \n",
    "    _map = np.empty([len(the_coords), 3])\n",
    "    for i,_part in enumerate(the_coords):\n",
    "        label = np.where(prop_index==the_properties[int(_part)])[0][0]       \n",
    "        _map[int(_part)] = [ the_coords[_part][0], the_coords[_part][1],label]\n",
    "    plt.figure(figsize=[10,4])\n",
    "    sns.kdeplot(data=_map[num_noise*2:,0], \n",
    "                data2=_map[num_noise*2:,1], \n",
    "                cmap = diverging_color_palette,\n",
    "                shade=True, \n",
    "                shade_lowest=True,\n",
    "                n_levels=500,\n",
    "               )\n",
    "    sc = sns.scatterplot(x=_map[:,0], \n",
    "                        y=_map[:,1], \n",
    "                        hue=[prop_index[i] for i in _map[:,2].astype(int)],\n",
    "                        palette = categorical_color_palette,\n",
    "                        s = [1 for j in range(num_noise*2)] + [10 for i in range(data_length)],\n",
    "                        linewidth=.1,\n",
    "                        )    \n",
    "    sc.set_xticklabels([])\n",
    "    sc.set_yticklabels([])\n",
    "    plt.title(\"{} - {}\".format(the_type,param))\n",
    "    if save_plot:\n",
    "        plt.savefig(os.path.join(the_data_path,\"{}-{}.png\".format(the_type,param)),dpi = 800,format = \"png\")\n",
    "    plt.show()\n",
    "#\n",
    "def plotMapIndex(the_type,the_coords, prop_index,the_properties,data_length,param=None, save_plot=False):\n",
    "    \"\"\"Scatterplot of clustered data points with engagement index gradients. Points with no index are not plotted.\"\"\"\n",
    "    \n",
    "    _map = np.empty([len(the_coords), 6])\n",
    "    for i,_part in enumerate(the_coords):\n",
    "        if i >= num_noise*2:\n",
    "            _map[int(_part)] = [ the_coords[_part][0], \n",
    "                                the_coords[_part][1], \n",
    "                                prop_index[i-(num_noise*2)][-1][0],\n",
    "                                prop_index[i-(num_noise*2)][-1][1],\n",
    "                                prop_index[i-(num_noise*2)][-1][2],\n",
    "                                prop_index[i-(num_noise*2)][-1][3]]\n",
    "        else:\n",
    "            _map[int(_part)] = [ the_coords[_part][0], the_coords[_part][1],0.,0.,0.,0.]\n",
    "    norm = [float(i)/max(list(_map[num_noise*2:,-1].flatten())) for i in _map[num_noise*2:,-1]]\n",
    "    plt.figure(figsize=[10,4])\n",
    "    sns.kdeplot(data=_map[num_noise*2:,0], \n",
    "                data2=_map[num_noise*2:,1], \n",
    "                cmap = diverging_color_palette,\n",
    "                shade=True, \n",
    "                shade_lowest=True,\n",
    "                n_levels=500\n",
    "               )\n",
    "    sc = sns.scatterplot(x=_map[num_noise*2:,0], \n",
    "                         y=_map[num_noise*2:,1], \n",
    "                         hue = norm,\n",
    "                         palette = index_color_palette,\n",
    "                         s = [0.1+(100.*norm[i]) for i in range(data_length)],\n",
    "                         linewidth=.1,\n",
    "                         alpha=1.\n",
    "                         )\n",
    "    sc.set_xticklabels([])\n",
    "    sc.set_yticklabels([])\n",
    "    plt.title(\"{} - {}\".format(the_type,param))\n",
    "    if save_plot:\n",
    "        plt.savefig(os.path.join(the_data_path,\"{}-{}.png\".format(the_type,param)),dpi = 800,format = \"png\")\n",
    "    plt.show()\n",
    "#\n",
    "def generateFakeData(category=\"same\",num=2000,size=6,dictParams={\"pos\":[.5,.5,.5],\"strength\":.61,\"substract\":25,\"size\":15.620499351813308}):\n",
    "    \"\"\"Generate fake data based on experiment params and random noise to compare with discriminated data.\"\"\"\n",
    "    \n",
    "    dist_factor = 1.\n",
    "    resolution = 100\n",
    "    all_tokens = []\n",
    "    for j in range(num):\n",
    "        radius = (dictParams[\"strength\"]/dictParams[\"substract\"])**0.5 * dist_factor\n",
    "        new_pos_x = dictParams[\"pos\"][0]\n",
    "        new_pos_y = dictParams[\"pos\"][1]\n",
    "        new_pos_z = dictParams[\"pos\"][2]\n",
    "        tokens = [[new_pos_x,new_pos_y,new_pos_z,dictParams[\"strength\"],dictParams[\"substract\"],dictParams[\"size\"]]]\n",
    "        for i in range(size-1):\n",
    "            if category != \"same\":\n",
    "                the_ratio = random.random()\n",
    "                the_radius = radius * the_ratio\n",
    "                newStrength = dictParams[\"strength\"] * the_ratio\n",
    "                newSubstract = dictParams[\"substract\"] * the_ratio\n",
    "            else:\n",
    "                the_radius = radius\n",
    "                newStrength = dictParams[\"strength\"]\n",
    "                newSubstract = dictParams[\"substract\"]  \n",
    "            theta = 2.*random.random()*math.pi\n",
    "            phi = 2.*math.asin(math.sqrt(random.random()))\n",
    "            new_pos_x = new_pos_x + (the_radius*math.sin(phi)*math.cos(theta))\n",
    "            new_pos_y = new_pos_y + (the_radius*math.sin(phi)*math.sin(theta)) \n",
    "            new_pos_z = new_pos_z + (the_radius*math.cos(phi))\n",
    "            newSize = resolution * math.sqrt(dictParams[\"strength\"]/dictParams[\"substract\"])\n",
    "            newToken = [new_pos_x,new_pos_y,new_pos_z,newStrength,newSubstract,newSize]\n",
    "            tokens.append(newToken)\n",
    "        all_tokens.append(np.asarray(tokens))\n",
    "        \n",
    "    return all_tokens\n",
    "#\n",
    "def getBandPower(data, sf, band, window_sec=None, relative=True):\n",
    "    \"\"\"Compute the average power, absolute or relative, of the signal x in a specific frequency band.\"\"\"\n",
    "\n",
    "    band = np.asarray(band)\n",
    "    low, high = band\n",
    "    # periodogram\n",
    "    psd, freqs = psd_array_multitaper(data, sf, adaptive=True,normalization='full', verbose=0)\n",
    "    # Frequency resolution\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "    # Find index of band in frequency vector\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "    # Integral approximation of the spectrum using parabola (Simpson's rule)\n",
    "    bp = simps(psd[:,:,idx_band], dx=freq_res)\n",
    "    if relative:\n",
    "        bp /= simps(psd, dx=freq_res)\n",
    "    return bp\n",
    "#\n",
    "def rawToEpochs(the_raw_path):\n",
    "    \"\"\"Get all *.fif files from given path and concatenates them to return a MNE Epochs object.\"\"\"\n",
    "    \n",
    "    raw = []\n",
    "    # dirs and concatenate saved data as raw\n",
    "    file_list = os.listdir(the_raw_path)\n",
    "    for _f in file_list:\n",
    "        if \".fif\" in _f:\n",
    "            print(\"   found file:{}\".format(_f))\n",
    "            read_fif = read_epochs(os.path.join(the_raw_path,_f),preload=False, verbose=False)\n",
    "            the_times = read_fif.times\n",
    "            the_zero_ind = np.where(the_times == 0)[0][0]\n",
    "            the_data = read_fif.get_data()[:,:,the_zero_ind]\n",
    "            the_events = read_fif.events[:, -1]\n",
    "            the_events = the_events.reshape(the_events.shape[0],1)\n",
    "            the_channels = read_fif.ch_names\n",
    "            the_data = np.concatenate((the_data,the_events), axis=1)\n",
    "            channel_types = ['eeg'] * len(list(the_channels)) + ['stim']\n",
    "            info = create_info(ch_names = the_channels + ['Stim'], \n",
    "                               ch_types = channel_types,\n",
    "                               sfreq = the_freq, \n",
    "                               montage = the_montage)\n",
    "            raw.append(RawArray(data = the_data.T, info = info,verbose =False))\n",
    "    if len(raw)>0:\n",
    "        the_raw = concatenate_raws(raw)\n",
    "        # rebuild epochs\n",
    "        try:\n",
    "            the_events = find_events(the_raw,\n",
    "                                     stim_channel=\"Stim\",\n",
    "                                     initial_event=True,\n",
    "                                     consecutive=True,\n",
    "                                     shortest_event=1,\n",
    "                                     output='onset',\n",
    "                                     verbose = False,)\n",
    "            the_epochs = Epochs(the_raw,\n",
    "                                events = the_events, \n",
    "                                event_id = the_markers,\n",
    "                                tmin = -0.1, tmax = 0.7,\n",
    "                                baseline = None,\n",
    "                                #reject = {'eeg': 75e-6},\n",
    "                                preload = True,\n",
    "                                verbose = False,\n",
    "                                picks = list(range(len(the_channels)))\n",
    "                               )\n",
    "            return the_epochs\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESHAPING DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles = []\n",
    "properties = []\n",
    "the_data = []\n",
    "index_len = 0\n",
    "# generate random samples\n",
    "noise_data_same = generateFakeData(category=\"same\",num=num_noise)\n",
    "noise_data_random = generateFakeData(category=\"random\",num=num_noise)\n",
    "for i,_same in enumerate(noise_data_same):\n",
    "    the_data.append(_same)\n",
    "    properties.append(\"Q0_C1\")\n",
    "for j,_random in enumerate(noise_data_random):\n",
    "    the_data.append(_random)\n",
    "    properties.append(\"Q0_C2\")\n",
    "# get all files and data\n",
    "for root, subdirs, files in os.walk(the_data_path):\n",
    "    for dirs in subdirs:\n",
    "        the_class_path = os.path.join(root,dirs)\n",
    "        for subroot,subsubdirs, subfiles in os.walk(the_class_path):\n",
    "            for _dir in subsubdirs:\n",
    "                the_file_path = os.path.join(subroot,_dir)\n",
    "                file_list = os.listdir(the_file_path)\n",
    "                for _f in file_list:\n",
    "                    if \".json\" in _f:\n",
    "                        print(\"found dir: {}\".format(_dir))\n",
    "                        print(\"   found file: {}\".format(_f))\n",
    "                        # look for related epochs data and compute their relative bandpowers and engagement index\n",
    "                        rel_epochs = rawToEpochs(the_file_path)\n",
    "                        if rel_epochs is not None and rel_epochs.get_data().shape[0]>0:\n",
    "                            theta_bp = getBandPower(data = rel_epochs.get_data(),sf = the_freq,band = theta_band, relative=True)\n",
    "                            alpha_bp = getBandPower(data = rel_epochs.get_data(),sf = the_freq,band = alpha_band, relative=True)\n",
    "                            beta_bp = getBandPower(data = rel_epochs.get_data(),sf = the_freq,band = beta_band, relative=True)\n",
    "                            # task engagement index : beta + k.beta /(alpha + theta)\n",
    "                            t_eng_index = (beta_bp.mean() + (beta_bp.mean()*K_ratio)) / (alpha_bp.mean() + theta_bp.mean())\n",
    "                            print(\"   engagement index:{}\".format(t_eng_index))\n",
    "                            # append json file\n",
    "                            datafiles.append([\"{}\".format(_dir),os.path.join(subroot,_dir,_f),[theta_bp.mean(),alpha_bp.mean(),beta_bp.mean(),t_eng_index]])\n",
    "                            index_len += 1\n",
    "                        else:\n",
    "                            print(\"   engagement index:{}\".format(0))\n",
    "                            # append json file\n",
    "                            datafiles.append([\"{}\".format(_dir),os.path.join(subroot,_dir,_f),[0,0,0,0]])\n",
    "                        properties.append(\"{}\".format(dirs))\n",
    "                        #properties.append(\"discriminated\")               \n",
    "data_len = len(datafiles)\n",
    "print(\"found {} files\".format(data_len))\n",
    "print(\"found {} indices\".format(index_len))\n",
    "for filepath in datafiles:\n",
    "    the_data.append(getDataFile(filepath[1]))\n",
    "# define classes         \n",
    "prop_index = np.unique(properties)    \n",
    "# get max amount of elements in a parts in one file\n",
    "the_max_part = max(len(_p) for _p in the_data)\n",
    "# get the length of params (ie params) in one element\n",
    "the_part_len = len(the_data[0][0])\n",
    "# equalize parts elements num with empty elements\n",
    "the_filler = [0. for i in range(the_part_len)]\n",
    "for i,_p in enumerate(the_data):\n",
    "    if len(_p) < the_max_part:\n",
    "        for j in range(the_max_part-len(_p)):\n",
    "            the_data[i] = np.vstack((the_data[i],the_filler))\n",
    "the_stacked_data = np.empty([len(the_data),the_max_part,the_part_len])\n",
    "for i,_r in enumerate(the_stacked_data):\n",
    "    the_stacked_data[i] = the_data[i]\n",
    "\n",
    "# normalize data and concatenate part elements\n",
    "the_stacked_data = normalize(the_stacked_data.reshape(the_stacked_data.shape[0],-1),norm=\"l2\",axis=1).reshape(the_stacked_data.shape)\n",
    "the_stacked_data = the_stacked_data.reshape(the_stacked_data.shape[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metric_type == \"all\":\n",
    "    the_metrics = all_metrics\n",
    "else:\n",
    "    the_metrics = single_metric\n",
    "# try with TSNE and UMAP on different perplexity and metric params\n",
    "for _m in the_metrics:\n",
    "    try:\n",
    "        print(\"TSNE --------------------------- metric: {}\".format(_m)) \n",
    "        for i in range(n_neighbors[0],n_neighbors[1],5):\n",
    "            mapped_T = applyTSNE(np.copy(the_stacked_data),i,_m)\n",
    "            plotMap(\"TSNE - {}\".format(_m),mapped_T, prop_index,properties,data_len,i,True)\n",
    "            plotMapIndex(\"INDEX-tsne - {}\".format(_m),mapped_T,datafiles,properties,data_len,i,True)\n",
    "    except Exception as e:\n",
    "        print(\"  Error.Doing the next one...\")\n",
    "        print(e)\n",
    "        continue\n",
    "    try:\n",
    "        print(\"UMAP --------------------------- metric: {}\".format(_m)) \n",
    "        for i in range(n_neighbors[0],n_neighbors[1],5):\n",
    "            mapped_U = applyUMAP(np.copy(the_stacked_data),i,_m)\n",
    "            plotMap(\"UMAP - {}\".format(_m),mapped_T, prop_index,properties,data_len,i,True)\n",
    "            plotMapIndex(\"INDEX-umap - {}\".format(_m),mapped_T,datafiles,properties,data_len,i,True)\n",
    "    except Exception as e:\n",
    "        print(\"  Error.Doing the next one...\")\n",
    "        print(e)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
